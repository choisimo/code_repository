# 공유 메모리와 프로듀서-컨슈머 패턴 문제 및 해설

## 문제 1: 버퍼 용량 제한
**문제**: 이미지에 나온 공유 버퍼 코드에서 `BUFFER_SIZE`가 10으로 정의되어 있지만, 실제로는 9개의 요소만 저장할 수 있습니다. 그 이유는 무엇인가요?

**해설**: 
코드에서는 원형 버퍼(circular buffer)를 사용하여 공유 메모리를 구현하고 있습니다. 두 개의 포인터 `in`과 `out`을 사용하여 버퍼의 입력 위치와 출력 위치를 추적합니다. 실제로 버퍼가 가득 찼는지 확인하는 조건문은 `((in + 1) % BUFFER_SIZE) == out`입니다[1].

만약 모든 10개 요소를 사용한다면, `in`과 `out`이 같은 값을 가질 수 있어서 버퍼가 비어있는 상태(`in == out`)와 버퍼가 가득 찬 상태를 구분할 수 없게 됩니다. 따라서 한 개의 슬롯을 항상 비워둠으로써 두 상태를 구분합니다[1].

## 문제 2: 공유 버퍼의 동기화 메커니즘
**문제**: 아래 코드가 세마포어로 어떻게 동기화되는지 설명하고, 만약 세마포어를 사용하지 않는다면 어떤 문제가 발생할 수 있는지 설명하세요.

```c
while (((in + 1) % BUFFER_SIZE) == out)
    ; /* do nothing */
buffer[in] = next_produced;
in = (in + 1) % BUFFER_SIZE;
```

**해설**:
이 코드는 생산자(producer) 코드의 일부로, 버퍼에 데이터를 추가합니다[1]. 현재는 세마포어 없이 `while` 루프를 사용하여 버퍼가 가득 찼는지 확인하는 busy waiting 방식을 사용하고 있습니다.

세마포어를 사용한다면:
1. `WRITE_SEM`과 `READ_SEM` 두 개의 세마포어를 정의합니다.
2. 생산자는 쓰기 전에 `WRITE_SEM`을 획득(wait)하고, 쓴 후에 `READ_SEM`을 해제(signal)합니다.
3. 소비자는 읽기 전에 `READ_SEM`을 획득하고, 읽은 후에 `WRITE_SEM`을 해제합니다.

세마포어를 사용하지 않으면:
1. **Race condition**: 여러 생산자나 소비자가 동시에 같은 버퍼 위치에 접근할 수 있습니다.
2. **Lost updates**: 한 생산자가 버퍼에 데이터를 쓰는 동안 다른 생산자가 같은 위치에 쓰면 첫 번째 데이터가 손실됩니다.
3. **CPU 낭비**: busy waiting은 CPU 시간을 낭비합니다[3].

## 문제 3: 공유 메모리에서 포인터 사용의 문제점
**문제**: 다음 코드가 공유 메모리 세그먼트에서 왜 문제가 될 수 있는지 설명하세요.

```c
item *p;
p = &buffer[in]; // 공유 메모리 내부의 주소를 저장
```

**해설**:
공유 메모리 세그먼트는 다른 프로세스의 가상 주소 공간에서 다른 주소에 매핑될 수 있습니다[5]. 따라서 한 프로세스에서 유효한 포인터가 다른 프로세스에서는 완전히 다른 메모리 위치를 가리킬 수 있습니다.

올바른 접근 방법은:
1. 절대 주소 대신 상대적 오프셋 사용하기: `size_t offset = (char*)p - (char*)baseaddr;`
2. 다른 프로세스에서: `p = (item*)((char*)baseaddr + offset);`
3. 또는 인덱스 번호를 사용하여 참조하기(배열 인덱스)[5]

이렇게 함으로써 공유 메모리가 각 프로세스의 다른 주소에 매핑되더라도 올바르게 메모리를 참조할 수 있습니다.

## 문제 4: 블로킹과 논블로킹 메시지 전달
**문제**: 이미지에 나온 자료에 따르면 블로킹과 논블로킹 메시지 전달의 차이점은 무엇이며, 소비자 코드에서 어떻게 블로킹 동작이 구현되어 있는지 설명하세요.

```c
while (in == out)
    ; /* do nothing */
next_consumed = buffer[out];
out = (out + 1) % BUFFER_SIZE;
```

**해설**:
블로킹과 논블로킹 메시지 전달의 차이점[4]:
- **블로킹(동기식)**: 
  - 블로킹 전송: 송신자는 메시지가 수신될 때까지 차단됨
  - 블로킹 수신: 수신자는 메시지가 사용 가능할 때까지 차단됨
- **논블로킹(비동기식)**:
  - 논블로킹 전송: 송신자는 메시지를 전송하고 계속 진행
  - 논블로킹 수신: 수신자는 유효한 메시지를 받거나 널 메시지를 받음

주어진 소비자 코드에서는 블로킹 수신이 `while (in == out)` 루프로 구현되어 있습니다[1]. 이 루프는 버퍼가 비어있는 경우(`in == out`) 메시지가 도착할 때까지(생산자가 `in` 값을 변경할 때까지) 계속 대기합니다. 이것은 busy waiting 방식의 블로킹 구현입니다.

더 효율적인 블로킹 구현은 세마포어나 조건 변수를 사용하여 CPU를 낭비하지 않는 방식으로 대기하는 것입니다.

## 문제 5: 공유 메모리 구현 방식 비교
**문제**: 검색 결과에 따르면 프로세스 간 메모리 공유를 위한 여러 메커니즘이 있습니다. System V 공유 메모리, 공유 파일 매핑, POSIX 공유 메모리 객체의 주요 차이점과 각각의 장단점을 설명하세요.

**해설**:
세 가지 주요 공유 메모리 메커니즘의 비교[5][6]:

1. **System V 공유 메모리**:
   - `shmget()`과 `shmat()` 함수 사용
   - 정수 키로 식별됨
   - 오래된 인터페이스지만 널리 지원됨
   - 제한된 기능 세트

2. **공유 파일 매핑(mmap)**:
   - `mmap()` 함수 사용하여 파일을 메모리에 매핑
   - 파일 기반으로 식별됨
   - 파일 내용으로 초기화되고 변경 사항이 파일에 반영됨
   - 시스템 재시작 후에도 데이터 유지 가능
   - 메모리 매핑된 I/O로 사용 가능

3. **POSIX 공유 메모리 객체**:
   - `shm_open()`과 `mmap()` 함수 조합 사용
   - 이름으로 식별됨
   - 기본 디스크 파일 없이 메모리 공유 가능
   - 더 현대적인 인터페이스

장단점:
- System V: 이식성이 좋지만 다른 메커니즘보다 유연성이 떨어짐
- 공유 파일 매핑: 데이터 지속성이 필요한 경우 유용하지만 디스크 파일 필요
- POSIX 공유 메모리: 디스크 파일 오버헤드 없이 메모리 공유 가능하지만 System V보다 덜 광범위하게 지원됨

일반적으로 새 애플리케이션에서는 더 현대적이고 유연한 공유 파일 매핑이나 POSIX 공유 메모리를 선호합니다[6].

## 문제 6: 공유 메모리 버퍼의 원형 구현
**문제**: 아래 생산자와 소비자 코드에서 원형 버퍼가 어떻게 구현되는지 설명하고, 이 구현이 무한 크기의 배열보다 어떤 이점이 있는지 설명하세요.

```c
// 생산자
buffer[in] = next_produced;
in = (in + 1) % BUFFER_SIZE;

// 소비자
next_consumed = buffer[out];
out = (out + 1) % BUFFER_SIZE;
```

**해설**:
원형 버퍼는 모듈로 연산(`% BUFFER_SIZE`)을 사용하여 버퍼의 끝에 도달했을 때 다시 처음으로 돌아가는 방식으로 구현됩니다[1]. 인덱스 `in`과 `out`이 증가하면서 `BUFFER_SIZE`에 도달하면 다시 0으로 돌아갑니다.

이 구현의 이점:
1. **메모리 효율성**: 고정된 크기의 메모리만 사용하므로 메모리를 효율적으로 활용합니다.
2. **재사용성**: 동일한 메모리 공간을 계속 재사용하므로 메모리 할당/해제 오버헤드가 없습니다.
3. **예측 가능한 성능**: 메모리 할당이 필요하지 않으므로 성능이 일정합니다.
4. **구현 단순성**: 복잡한 메모리 관리 없이 간단한 인덱스 계산만으로 구현됩니다.

무한 크기 배열 접근 방식은 실제로는 메모리 할당과 재할당이 필요하며, 이는 성능 병목과 메모리 단편화를 초래할 수 있습니다.


--- 

# 프로세스 간 통신(IPC): 공유 메모리 및 메시지 큐 문제집

Linux와 UNIX 시스템에서 프로세스 간 통신(IPC)의 핵심 기법인 공유 메모리와 메시지 큐에 관한 문제와 해설을 준비했습니다. 특히 헷갈리기 쉬운 개념과 코드 부분을 중점적으로 다루었습니다.

## POSIX 공유 메모리 (Shared Memory)

### 문제 1: 공유 메모리 생성 함수 이해
**Q:** 다음 POSIX 공유 메모리 코드에서 `shm_open()` 함수의 세 번째 매개변수인 `0666`의 의미는 무엇인가?
```c
shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666);
```

1. 파일 크기를 666 바이트로 설정한다
2. 파일의 접근 권한(permission)을 설정한다
3. 최대 666개의 프로세스가 접근할 수 있음을 의미한다
4. 타임아웃을 666밀리초로 설정한다

**A:** 2번. `0666`은 파일의 접근 권한(permission)을 설정하는 것으로, 소유자, 그룹, 그리고 다른 사용자 모두에게 읽기 및 쓰기 권한(rw-rw-rw-)을 부여합니다. 이는 리눅스/유닉스 파일 시스템의 권한 체계를 따르며, 공유 메모리 세그먼트가 파일 시스템에 표현되기 때문에 이러한 권한이 필요합니다.

### 문제 2: 공유 메모리 크기 설정
**Q:** 다음 코드는 어떤 기능을 수행하는지 설명하시오.
```c
ftruncate(shm_fd, 4096);
```

**A:** 이 코드는 공유 메모리 객체의 크기를 4096 바이트(4KB)로 설정합니다. `ftruncate()` 함수는 파일 디스크립터로 참조되는 파일(여기서는 공유 메모리 객체)의 크기를 변경하는 데 사용됩니다. 공유 메모리를 사용하기 전에 그 크기를 명시적으로 설정해야 하며, 이 설정이 없으면 메모리 매핑 시 오류가 발생할 수 있습니다.

### 문제 3: mmap 함수 이해
**Q:** 다음 코드에서 `MAP_SHARED` 플래그의 역할은 무엇인가?
```c
ptr = mmap(0, SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0);
```

1. 메모리 매핑을 여러 프로세스가 공유할 수 있게 한다
2. 같은 프로세스 내에서만 공유할 수 있게 한다
3. 파일 매핑을 비공개(private)로 만든다
4. 매핑을 읽기 전용으로 설정한다

**A:** 1번. `MAP_SHARED` 플래그는 메모리 매핑을 여러 프로세스 간에 공유할 수 있도록 설정합니다. 이 플래그를 사용하면 한 프로세스에서 매핑된 메모리 영역을 변경했을 때 동일한 공유 메모리를 매핑한 다른 프로세스도 그 변경 사항을 볼 수 있습니다. 이는 IPC(Inter-Process Communication)의 핵심 메커니즘입니다.

### 문제 4: 공유 메모리 생성과 접근
**Q:** 공유 메모리에 접근하기 위해 소비자 프로세스(consumer)가 수행해야 하는 과정을 올바른 순서로 나열하시오.

A. `shm_open()`으로 기존 공유 메모리 세그먼트 열기  
B. `mmap()`으로 프로세스 주소 공간에 매핑하기  
C. 매핑된 포인터를 통해 데이터 읽기  
D. `shm_unlink()`로 공유 메모리 제거하기  

**A:** 올바른 순서: A → B → C → D

공유 메모리 소비자는 먼저 `shm_open()`으로 생산자가 생성한 공유 메모리 세그먼트를 열고, `mmap()`을 사용하여 자신의 주소 공간에 매핑한 후, 반환된 포인터를 통해 데이터를 읽습니다. 마지막으로 더 이상 필요하지 않으면 `shm_unlink()`를 호출하여 공유 메모리를 제거합니다.

### 문제 5: 공유 메모리 오류 처리
**Q:** 다음 코드에서 오류가 있는 부분을 찾고 설명하시오.
```c
shm_fd = shm_open(name, O_RDONLY, 0666);
// 오류 검사 생략
ftruncate(shm_fd, 4096);  // 소비자가 크기 설정 시도
ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0);
```

**A:** 오류는 소비자 프로세스가 `O_RDONLY` 모드로 공유 메모리를 연 후 `ftruncate()`를 호출하는 부분입니다. `ftruncate()`는 파일 크기를 변경하는 작업으로 쓰기 권한이 필요합니다. 그러나 `shm_open()`에 `O_RDONLY` 플래그만 사용했기 때문에 쓰기 권한이 없어 `ftruncate()`가 실패합니다. 

소비자는 일반적으로 크기 조정을 할 필요가 없으며, 이미 생산자에 의해 설정된 크기를 사용해야 합니다. 올바른 접근법은 소비자 코드에서 `ftruncate()` 호출을 제거하는 것입니다.

## POSIX 메시지 큐 (Message Queue)

### 문제 6: 메시지 큐 생성 이해
**Q:** 다음 메시지 큐 코드에서 `mq_open()` 함수의 첫 번째 매개변수가 `/firt_mq`로 시작하는 이유는 무엇인가?
```c
mq = mq_open("/firt_mq", O_RDWR | O_CREAT, 0666, &attr);
```

**A:** POSIX 메시지 큐의 이름은 반드시 '/'로 시작해야 합니다. 이는 POSIX 표준의 요구사항으로, 메시지 큐 이름이 파일 시스템 경로처럼 취급되기 때문입니다. 따라서 `/firt_mq`와 같이 슬래시로 시작하는 형식을 사용해야 합니다. 이 이름은 메시지 큐를 고유하게 식별하는 데 사용되며, 다른 프로세스가 동일한 메시지 큐에 접근할 때 사용됩니다.

### 문제 7: 메시지 큐와 공유 메모리 비교
**Q:** 메시지 큐와 공유 메모리의 가장 큰 차이점은 무엇인가?

1. 메시지 큐는 단방향 통신만 지원하지만, 공유 메모리는 양방향 통신이 가능하다
2. 메시지 큐는 구조화된 데이터를 전송하지만, 공유 메모리는 원시 바이트만 전송한다
3. 메시지 큐는 비동기적 통신을 제공하지만, 공유 메모리는 프로세스 간 직접 메모리 공유를 제공한다
4. 메시지 큐는 windows에서만 지원되지만, 공유 메모리는 모든 운영체제에서 지원된다

**A:** 3번. 메시지 큐는 비동기적 통신 메커니즘으로, 한 프로세스가 메시지를 큐에 넣으면 다른 프로세스가 나중에 그것을 꺼낼 수 있습니다. 반면, 공유 메모리는 여러 프로세스가 동일한 메모리 영역에 직접 접근할 수 있게 해주는 방식입니다. 공유 메모리는 일반적으로 더 높은 성능을 제공하지만, 동기화 메커니즘이 별도로 필요합니다. 메시지 큐는 내장된 동기화 기능이 있어 사용이 더 간단할 수 있습니다.

### 문제 8: 메시지 큐 속성 구조체
**Q:** 다음 `mq_attr` 구조체의 각 필드가 의미하는 것을 설명하시오.
```c
struct mq_attr {
    long mq_flags;    // 메시지 큐 플래그
    long mq_maxmsg;   // 최대 메시지 수
    long mq_msgsize;  // 최대 메시지 크기
    long mq_curmsgs;  // 현재 메시지 수
};
```

**A:** `mq_attr` 구조체의 필드 의미:
- `mq_flags`: 메시지 큐의 동작 방식을 결정하는 플래그. 예를 들어, 논블로킹 모드(O_NONBLOCK)를 설정할 수 있습니다.
- `mq_maxmsg`: 큐에 저장될 수 있는 최대 메시지 수. 시스템 제한이 있으며, 리소스 관리에 중요합니다.
- `mq_msgsize`: 각 메시지의 최대 크기(바이트). 이 크기보다 큰 메시지는 전송할 수 없습니다.
- `mq_curmsgs`: 현재 큐에 있는 메시지 수. 이 필드는 읽기 전용으로, `mq_getattr()`로 조회할 때만 유의미합니다.

### 문제 9: 메시지 수신 함수 이해
**Q:** 다음 `mq_receive()` 함수 호출에서 네 번째 매개변수 `NULL`의 의미는 무엇인가?
```c
if(mq_receive(mq, buf, attr.mq_msgsize, NULL) == -1){
    perror("mq_receive error");
    exit(-1);
}
```

1. 메시지 우선순위를 무시한다
2. 메시지 우선순위를 저장할 변수가 없다
3. 수신된 메시지를 NULL로 설정한다
4. 비동기 수신 모드를 활성화한다

**A:** 2번. `mq_receive()` 함수의 네 번째 매개변수는 수신된 메시지의 우선순위를 저장할 포인터입니다. `NULL`을 전달하면 메시지 우선순위 정보를 무시하겠다는 의미입니다. 즉, 메시지의 내용만 관심이 있고 우선순위는 필요하지 않을 때 `NULL`을 사용합니다. 우선순위 정보가 필요하다면 `unsigned int` 타입 변수의 주소를 전달해야 합니다.

### 문제 10: 메시지 전송 함수 이해
**Q:** 다음 `mq_send()` 함수 호출에서 마지막 매개변수 `1`의 의미는 무엇인가?
```c
if((mq_send(mq, buf, strlen(buf)+1, 1)) == -1){
    perror("mq_send error");
    exit(-1);
}
```

**A:** `mq_send()` 함수의 마지막 매개변수 `1`은 전송하는 메시지의 우선순위를 나타냅니다. POSIX 메시지 큐는 우선순위 기반 큐로, 높은 우선순위의 메시지가 먼저 전달됩니다. 우선순위 값은 0이 가장 낮고, 시스템에서 정의된 최대값(일반적으로 MQ_PRIO_MAX-1)까지 설정할 수 있습니다. 이 코드에서는 우선순위를 1로 설정하여, 우선순위 0인 메시지보다 먼저 처리되도록 했습니다.

## 파이프(Pipes)와 기타 IPC 비교

### 문제 11: 일반 파이프와 공유 메모리 비교
**Q:** 일반 파이프(Ordinary Pipes)와 공유 메모리(Shared Memory)의 차이점으로 옳은 것은?

1. 파이프는 양방향 통신이 가능하지만, 공유 메모리는 단방향 통신만 가능하다
2. 파이프는 부모-자식 관계가 필요하지만, 공유 메모리는 관계없는 프로세스 간에도 사용 가능하다
3. 파이프는 네트워크를 통한 통신이 가능하지만, 공유 메모리는 같은 기계 내에서만 사용 가능하다
4. 파이프는 구조화된 데이터 전송만 가능하지만, 공유 메모리는 모든 형태의 데이터를 전송할 수 있다

**A:** 2번. 일반 파이프는 부모-자식 프로세스 관계가 필요하지만, 공유 메모리는 관계없는 프로세스 간에도 사용 가능합니다. 일반 파이프는 `pipe()` 시스템 콜로 생성되며, 이를 사용하는 프로세스들 사이에 부모-자식 관계가 있어야 합니다. 반면 공유 메모리는 `shm_open()`을 통해 생성하고, 이름으로 식별되기 때문에 관계없는 프로세스 간에도 통신이 가능합니다.

### 문제 12: 명명된 파이프와 메시지 큐 비교
**Q:** 명명된 파이프(Named Pipes)와 메시지 큐(Message Queue)의 공통점은?

1. 둘 다 단방향 통신만 지원한다
2. 둘 다 부모-자식 관계가 필요하다
3. 둘 다 프로세스 간 관계 없이 통신이 가능하다
4. 둘 다 네트워크를 통한 통신을 지원한다

**A:** 3번. 명명된 파이프와 메시지 큐 모두 프로세스 간 관계 없이 통신이 가능합니다. 두 메커니즘 모두 파일 시스템에 이름을 가진 엔터티로 표현되어, 서로 관련 없는 프로세스들이 해당 이름을 통해 접근할 수 있습니다. 명명된 파이프는 `mkfifo()`로 생성되고 일반 파일처럼 열 수 있으며, 메시지 큐는 `mq_open()`으로 생성하고 이름으로 식별합니다.

### 문제 13: IPC 메커니즘 종합 비교
**Q:** 다음 중 IPC 메커니즘의 특성에 관한 설명으로 잘못된 것은?

1. 공유 메모리는 데이터 복사가 발생하지 않아 가장 빠른 IPC 메커니즘이다
2. 메시지 큐는 내장된 동기화 메커니즘을 제공한다
3. 일반 파이프는 양방향 통신을 위해 두 개의 파이프가 필요하다
4. 공유 메모리는 사용자가 직접 동기화를 처리해야 하지만, 일반 파이프는 동기화가 필요 없다

**A:** 4번. 공유 메모리는 사용자가 직접 동기화를 처리해야 하는 것은 맞지만, 일반 파이프도 동기화가 필요 없다는 것은 틀렸습니다. 파이프는 내장된 동기화 메커니즘이 있지만, 특정 상황(예: 다중 생산자 또는 소비자)에서는 추가 동기화가 필요할 수 있습니다. 또한 파이프의 버퍼가 가득 찼을 때 쓰기 작업은 블록되고, 비어있을 때 읽기 작업이 블록되는 동기화 특성이 있습니다.

### 문제 14: 코드 디버깅 문제
**Q:** 다음 공유 메모리 코드에서 발생할 수 있는 문제점을 모두 찾으시오.
```c
#include 
#include 
#include 
#include 
#include 
#include 

int main() {
    const char *name = "OS";
    const int SIZE = 4096;
    int shm_fd;
    void *ptr;
    
    // 이미 존재하는 공유 메모리 열기
    shm_fd = shm_open(name, O_RDONLY, 0);
    if (shm_fd == -1) {
        printf("Shared memory failed\n");
        exit(-1);
    }
    
    // 크기 설정 시도
    ftruncate(shm_fd, SIZE);
    
    // 매핑
    ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0);
    
    // 읽기 시도
    printf("Data: %s\n", (char *)ptr);
    
    return 0;
}
```

**A:** 이 코드에는 여러 문제점이 있습니다:

1. `exit()` 함수 사용 시 `` 헤더가 포함되지 않았습니다.
2. `O_RDONLY` 모드로 공유 메모리를 열었지만, `ftruncate()`를 호출하여 크기를 변경하려 합니다. 이는 쓰기 권한이 없어 실패합니다.
3. `PROT_WRITE` 보호 플래그로 메모리를 매핑하려 하지만, 파일 디스크립터는 읽기 전용으로 열려 있어 권한 충돌이 발생합니다.
4. 읽기 시도를 하려면 `PROT_READ` 플래그가 필요하지만, `PROT_WRITE`만 설정되어 있습니다.
5. 공유 메모리 사용 후 `munmap()`이나 `shm_unlink()`를 호출하지 않아 자원 누수가 발생합니다.

### 문제 15: 실제 적용 시나리오
**Q:** 다음 시나리오에 가장 적합한 IPC 메커니즘을 선택하고 그 이유를 설명하시오.

"대량의 이미지 데이터(약 100MB)를 처리하는 두 프로세스가 있습니다. 한 프로세스는 이미지를 로드하고 처리하며, 다른 프로세스는 처리된 데이터를 실시간으로 화면에 표시해야 합니다."

**A:** 이 시나리오에 가장 적합한 IPC 메커니즘은 공유 메모리입니다. 그 이유는:

1. 대용량 데이터(100MB)를 처리해야 하므로, 데이터 복사 오버헤드가 없는 공유 메모리가 효율적입니다.
2. 실시간으로 표시해야 하므로 가장 빠른 통신 방식이 필요합니다.
3. 파이프나 메시지 큐는 대용량 데이터를 전송할 때 성능이 저하될 수 있습니다.

그러나 공유 메모리를 사용할 때는 데이터 일관성을 유지하기 위해 세마포어나 뮤텍스와 같은 동기화 메커니즘을 함께 사용해야 합니다. 특히 한 프로세스가 데이터를 처리하는 동안 다른 프로세스가 접근하지 못하도록 해야 합니다.

## 연습용 코드 예제

공유 메모리 생산자-소비자 패턴의 간단한 구현 예제입니다. 이 코드를 분석하고 수정해보면서 공유 메모리의 개념을 더 깊이 이해할 수 있습니다.

```c
// producer.c
#include 
#include 
#include 
#include 
#include 
#include 
#include 
#include 

int main() {
    const char *name = "OS_SHARED_MEM";
    const int SIZE = 4096;
    int shm_fd;
    void *ptr;
    
    // 공유 메모리 생성
    shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666);
    if (shm_fd == -1) {
        perror("shm_open");
        exit(EXIT_FAILURE);
    }
    
    // 공유 메모리 크기 설정
    if (ftruncate(shm_fd, SIZE) == -1) {
        perror("ftruncate");
        exit(EXIT_FAILURE);
    }
    
    // 메모리 매핑
    ptr = mmap(0, SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0);
    if (ptr == MAP_FAILED) {
        perror("mmap");
        exit(EXIT_FAILURE);
    }
    
    // 공유 메모리에 데이터 쓰기
    sprintf(ptr, "Hello from producer!");
    printf("Written to shared memory: %s\n", (char *)ptr);
    
    // 사용자 입력 대기 (소비자가 실행될 시간을 주기 위해)
    printf("Press enter to exit...");
    getchar();
    
    // 정리
    if (munmap(ptr, SIZE) == -1) {
        perror("munmap");
        exit(EXIT_FAILURE);
    }
    
    if (close(shm_fd) == -1) {
        perror("close");
        exit(EXIT_FAILURE);
    }
    
    // 공유 메모리 객체 삭제는 소비자가 사용을 마친 후에 수행할 수 있음
    
    return 0;
}
```

```c
// consumer.c
#include 
#include 
#include 
#include 
#include 
#include 
#include 

int main() {
    const char *name = "OS_SHARED_MEM";
    const int SIZE = 4096;
    int shm_fd;
    void *ptr;
    
    // 공유 메모리 열기
    shm_fd = shm_open(name, O_RDONLY, 0666);
    if (shm_fd == -1) {
        perror("shm_open");
        exit(EXIT_FAILURE);
    }
    
    // 메모리 매핑
    ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0);
    if (ptr == MAP_FAILED) {
        perror("mmap");
        exit(EXIT_FAILURE);
    }
    
    // 공유 메모리에서 데이터 읽기
    printf("Read from shared memory: %s\n", (char *)ptr);
    
    // 사용자 입력 대기
    printf("Press enter to clean up...");
    getchar();
    
    // 정리
    if (munmap(ptr, SIZE) == -1) {
        perror("munmap");
        exit(EXIT_FAILURE);
    }
    
    if (close(shm_fd) == -1) {
        perror("close");
        exit(EXIT_FAILURE);
    }
    
    // 공유 메모리 객체 삭제
    if (shm_unlink(name) == -1) {
        perror("shm_unlink");
        exit(EXIT_FAILURE);
    }
    
    return 0;
}
```

이 코드들은 공유 메모리를 사용한 기본적인 IPC 구현을 보여주며, 실제 시스템에서는 동기화 메커니즘을 추가하여 데이터 일관성을 보장해야 합니다.



---


# IPC 및 네트워크 프로그래밍 심화 문제집

이 문제집은 운영체제에서 다루는 프로세스 간 통신(IPC)과 네트워크 프로그래밍에 관한 개념들을 심화적으로 이해하기 위한 것입니다. 각 주제별로 헷갈리기 쉽거나 틀리기 쉬운 개념들에 관한 문제들을 준비했습니다.

## 1. 파이프(Pipes)와 포크(Fork) 관련 문제

### 문제 1.1
**Q:** 다음 코드의 실행 결과로 가장 적절한 것은?
```c
#include 
#include 
#include 

int main() {
    int fd[2];
    pipe(fd);
    char write_msg[20] = "Hello";
    char read_msg[20];
    
    if (fork() == 0) {
        close(fd[0]);
        write(fd[1], write_msg, strlen(write_msg) + 1);
        close(fd[1]);
    } else {
        close(fd[1]);
        read(fd[0], read_msg, sizeof(read_msg));
        printf("Read: %s\n", read_msg);
        close(fd[0]);
    }
    return 0;
}
```

1. 출력 없음 (프로그램 종료)
2. `Read: Hello`
3. `Read: ` (빈 문자열)
4. 무한 대기 상태

**A:** 2번. `Read: Hello`
파이프는 단방향 통신 채널을 생성합니다. 자식 프로세스가 쓰기 끝(fd[1])으로 "Hello"를 쓰고, 부모 프로세스가 읽기 끝(fd)에서 이 데이터를 읽습니다. 부모 프로세스는 "Hello"를 읽고 출력하게 됩니다. 주의할 점은 각 프로세스가 사용하지 않는 파이프 끝을 닫는 것이 중요합니다.

### 문제 1.2
**Q:** 파이프를 사용할 때 다음 상황 중 교착 상태(deadlock)를 일으킬 수 있는 것은?

1. 자식 프로세스가 파이프의 읽기 끝과 쓰기 끝을 모두 닫은 경우
2. 부모 프로세스가 자식이 쓰기 전에 파이프에서 읽기를 시도하는 경우
3. 부모와 자식이 모두 파이프의 쓰기 끝만 열고 읽기 끝을 닫은 경우
4. 부모와 자식이 모두 파이프의 읽기 끝만 열고 쓰기 끝을 닫은 경우

**A:** 4번. 부모와 자식이 모두 파이프의 읽기 끝만 열고 쓰기 끝을 닫은 경우입니다.

파이프에서 읽기 연산은 파이프가 비어있으면 데이터가 도착할 때까지 차단됩니다. 그러나 쓰기 끝이 모두 닫혀 있다면, 읽기 연산은 즉시 EOF(파일의 끝)를 반환합니다. 따라서 3번은 교착 상태가 아니라 즉시 0(EOF)를 반환합니다. 그러나 4번의 경우, 양쪽 다 쓰기가 불가능하므로 데이터가 결코 파이프에 쓰여질 수 없고, 파이프가 비어있다면 양쪽 모두 영원히 읽기를 기다리게 됩니다.

### 문제 1.3
**Q:** 다음 중 익명 파이프(anonymous pipe)에 대한 설명으로 틀린 것은?

1. 익명 파이프는 관련 없는 프로세스 간에 통신하는 데 사용될 수 있다
2. 익명 파이프는 단방향 통신만 지원한다
3. 파이프를 생성한 프로세스가 종료되면 파이프도 소멸된다
4. 양방향 통신을 위해서는 두 개의 파이프가 필요하다

**A:** 1번. 익명 파이프는 관련 없는 프로세스 간에 통신하는 데 사용될 수 없습니다.

익명 파이프는 부모-자식 관계와 같이 공통 조상을 가진 프로세스 간에만 통신할 수 있습니다. 이는 파이프 생성 후 fork()를 통해 자식 프로세스가 파이프 디스크립터를 상속받기 때문입니다. 관련 없는 프로세스 간의 통신을 위해서는 명명된 파이프(named pipe 또는 FIFO) 또는 다른 IPC 메커니즘을 사용해야 합니다.

### 문제 1.4
**Q:** 파이프의 버퍼가 가득 찼을 때 파이프에 쓰기를 시도하는 프로세스는 어떻게 되는가?

1. 쓰기 작업은 버퍼가 비워질 때까지 차단(block)된다
2. EAGAIN 오류가 반환된다
3. 데이터 손실이 발생한다
4. 파이프가 자동으로 확장되어 모든 데이터를 수용한다

**A:** 1번. 쓰기 작업은 버퍼가 비워질 때까지 차단(block)됩니다.

기본적으로 파이프 쓰기 작업은 차단 모드에서 수행됩니다. 즉, 파이프의 버퍼가 가득 찼을 때 쓰기를 시도하면 파이프의 다른 끝에서 읽기가 발생하여 버퍼 공간이 생길 때까지 프로세스가 차단됩니다. 파이프를 비차단 모드(O_NONBLOCK)로 설정하면 버퍼가 가득 찼을 때 EAGAIN 오류가 반환될 수 있습니다.

### 문제 1.5
**Q:** 리눅스에서 파이프의 버퍼 크기를 알아내는 방법은?

1. `pipe_size` 시스템 콜 사용
2. `fcntl` 시스템 콜과 `F_GETPIPE_SZ` 명령어 사용
3. 파이프의 속성을 조회하는 `stat` 시스템 콜 사용
4. 버퍼 크기는 고정되어 있어 런타임에 알아낼 수 없다

**A:** 2번. `fcntl` 시스템 콜과 `F_GETPIPE_SZ` 명령어를 사용합니다.

리눅스에서는 파이프의 버퍼 크기를 알아내기 위해 `fcntl` 시스템 콜을 `F_GETPIPE_SZ` 명령어와 함께 사용할 수 있습니다:
```c
int size = fcntl(fd[0], F_GETPIPE_SZ);
```
또한 `F_SETPIPE_SZ` 명령어를 사용하여 파이프 버퍼 크기를 조정할 수도 있습니다. 리눅스의 기본 파이프 버퍼 크기는 일반적으로 65,536바이트(64KB)입니다.

## 2. 소켓(Sockets) 통신 모델

### 문제 2.1
**Q:** 소켓 통신에서 `bind()`, `listen()`, `accept()`의 올바른 호출 순서는?

1. `bind()` → `accept()` → `listen()`
2. `listen()` → `bind()` → `accept()`
3. `bind()` → `listen()` → `accept()`
4. `accept()` → `bind()` → `listen()`

**A:** 3번. `bind()` → `listen()` → `accept()`입니다.

서버 소켓 설정의 일반적인 순서는:
1. `socket()`: 소켓 생성
2. `bind()`: 소켓을 특정 IP 주소 및 포트에 바인딩
3. `listen()`: 소켓을 수동 모드로 전환하여 클라이언트 연결 대기 설정
4. `accept()`: 클라이언트 연결 수락 (이 함수는 연결이 들어올 때까지 차단됨)

이 순서를 따르지 않으면 오류가 발생합니다. 예를 들어 `bind()` 전에 `listen()`을 호출하거나 `listen()` 전에 `accept()`를 호출하면 실패하게 됩니다.

### 문제 2.2
**Q:** 다음 소켓 유형 중 연결 지향적이며 신뢰성 있는 통신을 제공하는 것은?

1. `SOCK_DGRAM`
2. `SOCK_STREAM`
3. `SOCK_RAW`
4. `SOCK_SEQPACKET`

**A:** 2번. `SOCK_STREAM`입니다.

`SOCK_STREAM`은 TCP(Transmission Control Protocol)를 사용하는 연결 지향적인 소켓으로, 신뢰성 있는 양방향 데이터 스트림을 제공합니다. 이 소켓 유형은 데이터가 순서대로, 오류 없이, 중복 없이 전달되도록 보장합니다. 반면, `SOCK_DGRAM`은 UDP(User Datagram Protocol)를 사용하는 비연결형 소켓으로 신뢰성은 낮지만 오버헤드가 적습니다.

### 문제 2.3
**Q:** TCP 연결에서 `TIME_WAIT` 상태의 목적은 무엇인가? (2개 선택)

1. 지연된 패킷이 다른 연결에 영향을 주는 것을 방지
2. 서버가 새 연결을 수락하기 전에 일정 시간 대기
3. 클라이언트가 연결 종료 요청을 재시도할 수 있는 시간 제공
4. 이전 연결의 중복된 패킷이 새 연결에 전달되는 것을 방지

**A:** 1번과 4번입니다.

`TIME_WAIT` 상태는 연결을 닫은 후 일정 시간(보통 2MSL, 최대 세그먼트 수명의 2배) 동안 해당 연결을 새롭게 사용하지 않도록 보장합니다. 이것의 주요 목적은:

1. 이전 연결의 지연된 패킷이 도착하여 새로운 동일한 주소/포트를 사용하는 연결에 영향을 주는 것을 방지(4번)
2. TCP의 "신뢰성 있는 연결 종료" 보장 - 최종 ACK가 손실된 경우 상대방이 FIN을 재전송할 수 있고, 이를 처리할 수 있도록 함(1번)

이는 TCP 프로토콜의 중요한 부분으로, 네트워크에서 지연된 패킷이 혼란을 주는 것을 방지합니다.

### 문제 2.4
**Q:** 다음 중 UDP 소켓이 TCP 소켓보다 더 적합한 애플리케이션은?

1. 웹 브라우저
2. 파일 전송 프로토콜
3. 실시간 비디오 스트리밍
4. 원격 로그인 프로토콜

**A:** 3번. 실시간 비디오 스트리밍입니다.

UDP는 다음과 같은 특성을 가진 애플리케이션에 더 적합합니다:
- 실시간 통신이 중요한 경우(지연 시간이 적음)
- 일부 데이터 손실이 허용되는 경우
- 빠른 데이터 전송이 필요한 경우
- 연결 설정 오버헤드를 줄이고 싶은 경우

실시간 비디오 스트리밍은 이러한 특성을 갖고 있습니다. 몇 프레임의 손실은 허용될 수 있으며, 패킷 재전송으로 인한 지연보다 적은 지연 시간이 더 중요합니다. 반면, 웹 브라우징, 파일 전송, 원격 로그인은 모든 데이터가 정확히 전달되어야 하므로 TCP가 더 적합합니다.

### 문제 2.5
**Q:** `socket()` 함수 호출 시 잘못된 도메인 또는 타입을 지정했을 때 반환되는 오류 코드는?

1. `EACCES`
2. `EINVAL`
3. `EMFILE`
4. `ENOTSOCK`

**A:** 2번. `EINVAL`입니다.

`socket()` 함수가 지원되지 않는 도메인(domain)이나 타입(type)으로 호출되면 `EINVAL` 오류를 반환합니다. 이는 "Invalid argument"(유효하지 않은 인자)를 의미합니다. 반면, `EACCES`는 권한 거부, `EMFILE`은 프로세스의 파일 디스크립터 테이블이 가득 찼을 때, `ENOTSOCK`은 소켓이 아닌 파일 디스크립터에 소켓 작업을 시도할 때 발생합니다.

## 3. 프로세스 간 통신(IPC) 메커니즘 비교

### 문제 3.1
**Q:** 다음 IPC 메커니즘 중 가장 높은 성능(데이터 전송 속도 측면)을 제공하는 것은?

1. 파이프(Pipes)
2. 메시지 큐(Message Queues)
3. 공유 메모리(Shared Memory)
4. 소켓(Sockets)

**A:** 3번. 공유 메모리(Shared Memory)입니다.

공유 메모리는 일반적으로 모든 IPC 메커니즘 중 가장 높은 성능을 제공합니다. 그 이유는:
- 커널 공간과 사용자 공간 사이의 데이터 복사가 필요 없음
- 시스템 호출 오버헤드 감소
- 프로세스들이 직접 동일한 메모리 영역에 접근

그러나 공유 메모리는 동기화 메커니즘(세마포어, 뮤텍스 등)을 별도로 구현해야 하는 복잡성이 있습니다. 파이프, 메시지 큐, 소켓은 모두 커널을 통한 데이터 복사가 필요하므로 더 많은 오버헤드가 발생합니다.

### 문제 3.2
**Q:** 다음 코드의 실행 결과로 가장 적절한 것은?
```c
#include 
#include 
#include 
#include 
#include 
#include 

int main() {
    const char *name = "OS";
    const int SIZE = 4096;
    int shm_fd;
    void *ptr;
    
    // 공유 메모리 열기
    shm_fd = shm_open(name, O_RDONLY, 0);
    if (shm_fd == -1) {
        printf("Shared memory failed\n");
        return -1;
    }
    
    // 크기 설정 시도
    ftruncate(shm_fd, SIZE);
    
    // 매핑
    ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0);
    
    // 읽기 시도
    printf("Data: %s\n", (char *)ptr);
    
    return 0;
}
```

1. "Data: " 다음에 공유 메모리의 내용 출력
2. "Shared memory failed" 메시지 출력
3. 세그먼테이션 오류(Segmentation fault) 발생
4. "Invalid argument" 오류 메시지 출력

**A:** 3번. 세그먼테이션 오류(Segmentation fault) 발생합니다.

이 코드에는 여러 문제가 있습니다:
1. 공유 메모리를 읽기 전용(`O_RDONLY`)으로 열었으나, `ftruncate()`를 호출하여 크기를 변경하려고 시도합니다. 이는 쓰기 권한이 없어 실패합니다.
2. 더 심각한 문제로, 메모리를 쓰기 보호(`PROT_WRITE`)로 매핑하려고 시도하지만, 파일 디스크립터는 읽기 전용으로 열려 있습니다. 이 권한 충돌로 `mmap()`은 실패하고 `MAP_FAILED`(-1)을 반환합니다.
3. 실패한 매핑에 대한 검사 없이 `ptr`을 역참조하려고 시도하므로 세그먼테이션 오류가 발생합니다.

올바른 코드는 `O_RDWR`로 열거나, `PROT_READ`로 매핑하고, 매핑 실패를 확인해야 합니다.

### 문제 3.3
**Q:** 다음 중 메시지 큐(Message Queue)에 대한 설명으로 옳지 않은 것은?

1. 메시지는 지정된 유형(type)으로 분류될 수 있다
2. 다양한 크기의 메시지를 지원한다
3. 메시지는 항상 FIFO(First-In-First-Out) 순서로 전달된다
4. 커널은 메시지를 버퍼링하므로 보내는 프로세스와 받는 프로세스가 동시에 실행될 필요가 없다

**A:** 3번. 메시지는 항상 FIFO(First-In-First-Out) 순서로 전달된다는 설명은 옳지 않습니다.

POSIX 메시지 큐와 System V 메시지 큐 모두 메시지 유형(type)을 기반으로 선택적 수신을 지원합니다. 즉, 수신자는 특정 유형의 메시지만 받을 수 있으며, 이로 인해 메시지가 큐에 들어온 순서와 다른 순서로 처리될 수 있습니다. 예를 들어, 우선순위가 높은 메시지 유형은 큐에 나중에 들어왔더라도 먼저 처리될 수 있습니다.

### 문제 3.4
**Q:** 다음 IPC 메커니즘 중 네트워크로 연결된 서로 다른 컴퓨터 간의 통신에 사용할 수 있는 것은?

1. UNIX 도메인 소켓
2. 공유 메모리
3. 메시지 큐
4. 인터넷 도메인 소켓

**A:** 4번. 인터넷 도메인 소켓입니다.

나열된 IPC 메커니즘 중 인터넷 도메인 소켓(TCP/IP 소켓)만이 네트워크로 연결된 서로 다른 컴퓨터 간의 통신에 사용될 수 있습니다. UNIX 도메인 소켓, 공유 메모리, 메시지 큐는 모두 단일 호스트 내의 프로세스 간 통신을 위한 메커니즘으로, 네트워크를 통한 통신을 지원하지 않습니다.

### 문제 3.5
**Q:** 다음 중 IPC 메커니즘을 사용할 때 별도의 동기화가 가장 필요한 것은?

1. 파이프
2. 메시지 큐
3. 공유 메모리
4. 소켓

**A:** 3번. 공유 메모리입니다.

공유 메모리는 가장 빠른 IPC 메커니즘이지만, 내장된 동기화 메커니즘이 없습니다. 공유 메모리를 사용하는 프로세스들은 데이터 무결성을 보장하고 경쟁 조건을 방지하기 위해 세마포어, 뮤텍스 또는 다른 동기화 메커니즘을 별도로 구현해야 합니다. 반면, 파이프, 메시지 큐, 소켓은 커널에 의해 관리되며 내장된 동기화 메커니즘(예: 버퍼링, 차단 동작)을 제공합니다.

## 4. 네트워크 프로그래밍 개념

### 문제 4.1
**Q:** IPv4 주소 `127.0.0.1`과 IPv6 주소 `::1`의 공통점은?

1. 모두 브로드캐스트 주소이다
2. 모두 루프백 주소이다
3. 모두 기본 게이트웨이 주소이다
4. 모두 임의의 호스트에 할당될 수 있다

**A:** 2번. 모두 루프백 주소입니다.

IPv4의 `127.0.0.1`과 IPv6의 `::1`은 모두 루프백 주소로, 같은 컴퓨터 내에서 자기 자신을 가리킵니다. 이 주소로 전송된 모든 데이터는 네트워크 어댑터를 통과하지 않고 로컬 컴퓨터로 다시 돌아옵니다. 주로 로컬 네트워크 서비스 테스트 및 개발에 사용됩니다. 두 주소 모두 "localhost"라는 이름으로 참조될 수 있습니다.

### 문제 4.2
**Q:** 소켓 프로그래밍에서 `htons()`, `ntohs()`, `htonl()`, `ntohl()` 함수의 목적은?

1. 호스트 이름과 IP 주소 간의 변환
2. 바이트 순서를 호스트 형식과 네트워크 형식 간에 변환
3. 소켓 연결 상태를 확인
4. IPv4와 IPv6 주소 간의 변환

**A:** 2번. 바이트 순서를 호스트 형식과 네트워크 형식 간에 변환합니다.

이 함수들은 다음과 같은 역할을 합니다:
- `htons()`: 호스트 바이트 순서에서 네트워크 바이트 순서로 short 정수 변환 (주로 포트 번호)
- `ntohs()`: 네트워크 바이트 순서에서 호스트 바이트 순서로 short 정수 변환
- `htonl()`: 호스트 바이트 순서에서 네트워크 바이트 순서로 long 정수 변환 (주로 IPv4 주소)
- `ntohl()`: 네트워크 바이트 순서에서 호스트 바이트 순서로 long 정수 변환

네트워크 바이트 순서는 항상 빅 엔디안이고, 호스트 바이트 순서는 CPU 아키텍처에 따라 다를 수 있기 때문에 이러한 변환이 필요합니다.

### 문제 4.3
**Q:** 소켓 통신에서 `SO_REUSEADDR` 옵션의 주요 목적은?

1. 다른 프로세스가 사용 중인 주소를 재사용하도록 함
2. 소켓이 닫힌 후에도 주소를 즉시 재사용할 수 있게 함
3. 로컬 주소를 재사용하여 다중 연결 지원
4. 네트워크 연결 복구를 위해 주소 재사용

**A:** 2번. 소켓이 닫힌 후에도 주소를 즉시 재사용할 수 있게 합니다.

`SO_REUSEADDR` 옵션의 주요 목적은 소켓이 정상적으로 닫힌 후에도 TCP의 TIME_WAIT 상태로 인해 바인딩할 수 없는 주소를 즉시 재사용할 수 있게 하는 것입니다. 이 옵션은 특히 서버를 재시작할 때 유용합니다. TIME_WAIT 상태는 일반적으로 2MSL(최대 세그먼트 수명의 2배) 동안 지속되며, 이 동안 해당 주소와 포트 조합은 보통 새로운 소켓에 바인딩될 수 없습니다.

### 문제 4.4
**Q:** 아래 네트워크 주소 중 올바른 형식의 IPv6 주소는?

1. `2001::1234::5678`
2. `2001:0db8:85a3:0000:0000:8a2e:0370:7334`
3. `2001:0db8:85a3::8a2e:0370:7334:1234`
4. `::ffff:192.168.1.1`

**A:** 4번. `::ffff:192.168.1.1`입니다.

IPv6 주소 형식에 관한 규칙:
- 2번은 올바른 IPv6 주소이지만, 연속된 0을 `::`로 압축할 수 있습니다.
- 1번은 틀린 형식입니다. IPv6 주소에서 `::` 축약은 한 번만 사용할 수 있습니다.
- 3번은 틀린 형식입니다. IPv6 주소는 8개의 16비트 블록(총 128비트)으로 구성되어야 하나, 이 주소는 9개의 블록을 갖고 있습니다.
- 4번은 올바른 IPv6 주소로, IPv4-매핑 IPv6 주소 형식입니다. 이 형식은 IPv4 주소를 IPv6 네트워크에서 표현하는 데 사용됩니다.

### 문제 4.5
**Q:** 클라이언트가 서버에 연결할 때 `connect()` 함수가 실패하면서 `ECONNREFUSED` 오류가 반환되는 가장 일반적인 원인은?

1. 서버가 실행 중이지만 해당 포트에서 리스닝하지 않음
2. 네트워크 연결이 끊어짐
3. 클라이언트가 잘못된 IP 주소를 사용함
4. 서버의 백로그 큐가 가득 참

**A:** 1번. 서버가 실행 중이지만 해당 포트에서 리스닝하지 않음입니다.

`ECONNREFUSED` 오류는 클라이언트가 연결하려는 대상 포트에서 리스닝하는 서버 프로세스가 없을 때 발생합니다. 이는 다음과 같은 상황에서 발생할 수 있습니다:
- 서버가 아직 시작되지 않았거나 종료됨
- 서버가 다른 포트에서 리스닝하고 있음
- 서버가 실행 중이지만 아직 `bind()`와 `listen()`을 호출하지 않음

네트워크 연결 문제는 일반적으로 `ETIMEDOUT`이나 `ENETUNREACH` 같은 다른 오류를 발생시킵니다. 백로그 큐가 가득 찬 경우는 일반적으로 연결이 대기열에 추가되거나 `ECONNREFUSED`가 아닌 다른 오류가 발생합니다.

## 5. 클라이언트-서버 아키텍처 및 RPC

### 문제 5.1
**Q:** RPC(Remote Procedure Call)의 주요 목적은?

1. 로컬 컴퓨터에서 원격 서버에 파일을 업로드하는 것
2. 분산 시스템에서 네트워크를 통해 다른 컴퓨터의 프로시저를 호출하는 것
3. 웹 서버와 브라우저 간의 통신을 표준화하는 것
4. 한 컴퓨터에서 다른 컴퓨터로 프로세스를 마이그레이션하는 것

**A:** 2번. 분산 시스템에서 네트워크를 통해 다른 컴퓨터의 프로시저를 호출하는 것입니다.

RPC는 클라이언트가 마치 로컬 함수를 호출하는 것처럼 네트워크를 통해 원격 서버의 프로시저를 호출할 수 있게 해주는 프로토콜입니다. 이는 분산 컴퓨팅에서 네트워크 통신의 복잡성을 추상화하여 프로그래머가 더 쉽게 분산 시스템을 구축할 수 있게 합니다. RPC는 인터페이스 정의, 매개변수 마샬링/언마샬링, 원격 호출 실행 메커니즘을 포함합니다.

### 문제 5.2
**Q:** 소켓 프로그래밍에서 클라이언트 측의 `connect()` 호출이 블록되는 동안 서버 측에서 `listen()` 백로그 큐가 가득 차면 어떤 일이 발생하는가?

1. 클라이언트의 `connect()`는 즉시 오류를 반환한다
2. 서버는 연결 요청을 자동으로 거부하고 클라이언트는 `ECONNREFUSED`를 받는다
3. 연결은 백로그 큐에 공간이 생길 때까지 TCP 프로토콜 수준에서 보류된다
4. 클라이언트 연결은 성공하지만 서버의 `accept()`가 호출될 때까지 데이터를 전송할 수 없다

**A:** 3번. 연결은 백로그 큐에 공간이 생길 때까지 TCP 프로토콜 수준에서 보류됩니다.

TCP 연결 설정 과정에서 백로그 큐가 가득 찬 경우:
- 일반적으로 운영체제는 추가 연결 요청(SYN 패킷)을 즉시 거부하지 않고 일정 시간 동안 무시합니다.
- 클라이언트의 TCP 스택은 응답이 없으면 SYN 패킷을 여러 번 재전송합니다.
- 이 동안 서버의 백로그 큐에 공간이 생기면(다른 연결이 `accept()`되면), 새 연결 요청이 수락될 수 있습니다.
- 여러 번의 재시도 후에도 연결이 설정되지 않으면 클라이언트는 `ETIMEDOUT` 오류를 받게 됩니다.

이 동작은 특히 DoS(Denial of Service) 공격에 대한 네트워크의 견고성에 영향을 미칩니다.

### 문제 5.3
**Q:** TCP 소켓에서 `listen()` 함수의 백로그(backlog) 매개변수는 무엇을 의미하는가?

1. 서버가 동시에 처리할 수 있는 최대 연결 수
2. 클라이언트 연결 요청을 대기시킬 수 있는 최대 큐 길이
3. 서버가 지원하는 최대 클라이언트 수
4. TCP 연결이 유휴 상태로 유지될 수 있는 최대 시간(초)

**A:** 2번. 클라이언트 연결 요청을 대기시킬 수 있는 최대 큐 길이입니다.

`listen()` 함수의 백로그 매개변수는 커널이 `accept()`를 기다리는 완료된 연결의 큐를 얼마나 크게 유지할지를 지정합니다. 서버가 연결을 충분히 빠르게 `accept()`하지 않으면, 새로운 클라이언트 연결은 이 큐에 배치됩니다. 큐가 가득 차면 새로운 연결 시도는 일반적으로 거부되거나 무시됩니다. 이것은 서버가 동시에 서비스할 수 있는 최대 연결 수나 클라이언트 수와는 다릅니다.

### 문제 5.4
**Q:** 다음 클라이언트-서버 아키텍처 패턴 중 각 연결에 대해 별도의 프로세스나 스레드를 생성하지 않고 단일 스레드에서 여러 클라이언트를 처리하는 것은?

1. 반복형 서버(Iterative Server)
2. 포크 서버(Forking Server)
3. 스레드 풀 서버(Thread Pool Server)
4. 이벤트 기반 서버(Event-driven Server)

**A:** 4번. 이벤트 기반 서버(Event-driven Server)입니다.

클라이언트-서버 아키텍처 패턴의 특징:
- 반복형 서버: 한 번에 하나의 클라이언트만 처리하며, 한 클라이언트의 요청을 완전히
처리한 후에 다음 클라이언트를 처리합니다.
- 포크 서버: 각 클라이언트 연결에 대해 새로운 프로세스를 생성합니다.
- 스레드 풀 서버: 미리 생성된 스레드 풀에서 각 클라이언트 연결에 스레드를 할당합니다.
- 이벤트 기반 서버: `select()`, `poll()`, `epoll()` 같은 I/O 멀티플렉싱 메커니즘을 사용하여 단일 스레드에서 여러 클라이언트 연결을 비동기적으로 처리합니다.

이벤트 기반 서버는 고성능 서버에서 자주 사용되는 패턴으로, 대량의 동시 연결을 처리할 수 있습니다.

### 문제 5.5
**Q:** TCP에서 "3-way handshake"의 주요 목적은?

1. 데이터 암호화를 위한 키 교환
2. 클라이언트와 서버 간의 연결 설정 및 초기 시퀀스 번호 동기화
3. 클라이언트와 서버 간의 전송 속도 협상
4. 서버 인증 및 클라이언트 권한 확인

**A:** 2번. 클라이언트와 서버 간의 연결 설정 및 초기 시퀀스 번호 동기화입니다.

TCP의 3-way handshake는 클라이언트와 서버 간에 연결을 설정하고 두 끝점이 서로의 존재를 확인하며 초기 시퀀스 번호(ISN)를 교환하는 과정입니다. 이 과정은:

1. 클라이언트가 SYN 패킷(시퀀스 번호 X)을 서버로 보냅니다.
2. 서버가 SYN-ACK 패킷(시퀀스 번호 Y, 확인 번호 X+1)으로 응답합니다.
3. 클라이언트가 ACK 패킷(시퀀스 번호 X+1, 확인 번호 Y+1)으로 응답합니다.

이 과정은 데이터 전송 전에 신뢰성 있는 연결을 설정하는 데 필수적이며, 시퀀스 번호 동기화는 TCP가 패킷 손실, 중복, 순서 변경을 감지하고 처리하는 데 중요합니다.

## 연습 문제 종합

이 문제들은 프로세스 간 통신 및 네트워크 프로그래밍의 다양한 측면을 다루며, 특히 헷갈리기 쉬운 개념들을 중점적으로 다루었습니다. 이러한 개념들을 정확히 이해하는 것은 실제 시스템 프로그래밍과 네트워크 애플리케이션 개발에 있어 중요합니다. 

각 주제에 대한 심층적인 이해를 위해 추가적인 실습과 코드 분석을 권장합니다. 특히 소켓 프로그래밍, 파이프 구현, 공유 메모리 사용 등은 직접 코드를 작성하고 동작을 관찰해 보는 것이 매우 도움이 됩니다.


--- 


# 프로세스와 스레드 관련 문제 및 해설

## 문제 1: 스레드 공유 자원

다음 중 프로세스 내의 여러 스레드가 **공유하지 않는** 자원은 무엇인가?

a) 코드(Code)
b) 전역 변수(Global variables)
c) 힙(Heap)
d) 레지스터(Registers)
e) 파일 디스크립터(File descriptors)

**해설**: 정답은 **d) 레지스터**입니다. 프로세스 내의 여러 스레드는 코드, 전역 변수, 힙 메모리, 파일 디스크립터 등을 공유합니다. 그러나 각 스레드는 자신만의 레지스터와 스택을 가지며, 이것들은 공유되지 않습니다. 이는 각 스레드가 독립적인 실행 흐름을 가질 수 있게 하는 필수 요소입니다. 레지스터는 CPU의 상태를 나타내며, 스레드 전환(thread switch) 시 이전 스레드의 레지스터 상태를 저장하고 새 스레드의 레지스터 상태를 복원합니다.

## 문제 2: 데이터 병렬성 vs 작업 병렬성

다음 시나리오 중 **데이터 병렬성(Data parallelism)**의 예로 가장 적합한 것은?

a) 웹 서버에서 여러 클라이언트 연결을 각각 다른 스레드로 처리
b) 워드 프로세서에서 철자 검사와 서식 지정을 동시에 수행
c) 대용량 배열의 각 부분을 여러 코어에 분배하여 동일한 합계 연산 수행
d) 비디오 인코딩 중 오디오와 비디오를 별도 스레드에서 처리
e) 게임 엔진에서 물리 계산과.렌더링을 서로 다른 스레드에서 실행

**해설**: 정답은 **c) 대용량 배열의 각 부분을 여러 코어에 분배하여 동일한 합계 연산 수행**입니다. 데이터 병렬성은 같은 데이터의 서로 다른 부분에 대해 동일한 연산을 병렬로 수행하는 방식입니다. 반면, 작업 병렬성(Task parallelism)은 서로 다른 작업을 서로 다른 스레드에서 수행하는 방식입니다. a), b), d), e)는 모두 서로 다른 작업을 병렬로 처리하는 작업 병렬성의 예입니다. 이미지 자료에서 보이듯이, 데이터 병렬성은 "데이터를 여러 개의 코어에 분배하여 각 코어가 같은 연산을 수행"하는 방식입니다.

## 문제 3: 암달의 법칙(Amdahl's Law) 계산

어떤 애플리케이션의 20%는 순차적으로 실행되어야 하고, 나머지 80%는 병렬화할 수 있다고 합니다. 이 애플리케이션을 4개의 코어에서 실행할 때, 이론적인 최대 속도 향상(speedup)은?

a) 2.0배
b) 2.5배
c) 3.5배
d) 4.0배
e) 5.0배

**해설**: 정답은 **b) 2.5배**입니다. 암달의 법칙은 다음 공식으로 표현됩니다:

Speedup ≤ 1 / (S + (1-S)/N)

여기서 S는 순차적 부분의 비율(0.2), N은 프로세서 수(4)입니다.

Speedup ≤ 1 / (0.2 + (1-0.2)/4)
Speedup ≤ 1 / (0.2 + 0.8/4)
Speedup ≤ 1 / (0.2 + 0.2)
Speedup ≤ 1 / 0.4
Speedup ≤ 2.5

이는 병렬화할 수 없는 순차적 부분(20%)이 전체 성능 향상의 한계를 결정한다는 것을 보여줍니다. 프로세서 수를 무한히 늘려도 최대 1/S(1/0.2 = 5배) 이상의 속도 향상은 얻을 수 없습니다.

## 문제 4: 병행성(Concurrency)과 병렬성(Parallelism)의 구분

다음 설명 중 **병행성(Concurrency)**이 아닌 **병렬성(Parallelism)**에 해당하는 것은?

a) 단일 코어에서 여러 스레드가 번갈아 실행되는 것
b) 여러 작업이 동시에 진행된다는 착각을 주는 것
c) 라운드 로빈 스케줄링을 통해 CPU 시간을 나누는 것
d) 멀티코어 시스템에서 동시에 여러 코드 경로가 실행되는 것
e) 문맥 교환(Context switching)을 통해 여러 작업을 처리하는 것

**해설**: 정답은 **d) 멀티코어 시스템에서 동시에 여러 코드 경로가 실행되는 것**입니다. 병행성(Concurrency)과 병렬성(Parallelism)은 자주 혼동되는 개념입니다:

- **병행성(Concurrency)**: 여러 작업을 번갈아가며 진행하여 동시에 실행되는 것처럼 보이게 하는 개념입니다. 단일 코어에서도 구현 가능하며, 문맥 교환과 시분할(time-sharing)을 통해 이루어집니다. a), b), c), e)는 모두 병행성의 특징입니다.

- **병렬성(Parallelism)**: 정말로 여러 작업을 물리적으로 동시에 실행하는 것으로, 여러 개의 코어나 프로세서가 필요합니다. d)는 병렬성의 예시로, 다중 코어에서 실제로 동시에 명령을 실행하는 것을 의미합니다.

자료의 그림에서 보듯이, 단일 코어 시스템에서는 T₁, T₂, T₃, T₄ 스레드가 번갈아 실행되는 병행성을 보이는 반면, 멀티코어 시스템에서는 각 코어가 동시에 다른 스레드를 실행하는 병렬성을 보입니다.

## 문제 5: 멀티프로세서 동기화 이슈

멀티프로세서 환경에서 다음과 같은 코드를 두 CPU가 동시에 실행하려고 할 때 발생할 수 있는 문제는?

```c
while ((in + 1) % BUFFER_SIZE == out)
    ; /* do nothing */
buffer[in] = next_produced;
in = (in + 1) % BUFFER_SIZE;
```

a) 무한 루프(Infinite loop)
b) 캐시 일관성(Cache coherence) 문제
c) 스래싱(Thrashing)
d) 데드락(Deadlock)
e) 우선순위 역전(Priority inversion)

**해설**: 정답은 **b) 캐시 일관성(Cache coherence) 문제**입니다. 이 코드는 생산자-소비자 문제에서 생산자 코드의 일부로, 공유 버퍼에 데이터를 추가하려고 합니다. 두 CPU가 이 코드를 동시에 실행할 때:

1. 각 CPU는 변수 `in`과 `out`을 자신의 캐시에 로드합니다.
2. 한 CPU가 `in` 값을 변경하더라도, 다른 CPU의 캐시에는 이 변경이 즉시 반영되지 않을 수 있습니다.
3. 이로 인해 두 CPU가 동일한 버퍼 위치에 데이터를 쓰거나, 버퍼 상태에 대한 불일치로 인해 데이터가 손실될 수 있습니다.

이러한 문제를 해결하기 위해 멀티프로세서 시스템은 캐시 일관성 프로토콜을 사용하지만, 그럼에도 불구하고 이런 코드는 적절한 동기화 메커니즘(mutex, 세마포어 등)을 사용하지 않으면 문제가 발생할 수 있습니다. 자료에서 언급된 TSL(Test and Set Lock) 명령이나 기타 원자적 연산을 사용하여 동기화해야 합니다.

## 문제 6: 스레드 매핑 모델

다음 중 **Many-to-One** 스레드 모델의 단점이 아닌 것은?

a) 한 스레드가 블로킹되면 모든 스레드가 블로킹됨
b) 멀티코어 시스템에서 병렬 실행이 제한됨
c) 커널에서 한 번에 하나의 스레드만 실행 가능
d) 제한된 동시성
e) 멀티스레드 애플리케이션의 성능 저하 가능성

**해설**: 해당 내용 중 Many-to-One 스레드 모델의 단점이 아닌 것은 없습니다. 모든 항목(a~e)이 Many-to-One 모델의 단점에 해당합니다. Many-to-One 모델은 여러 사용자 수준 스레드를 하나의 커널 스레드에 매핑하는 방식으로:

1. 한 스레드가 블로킹 호출을 하면 전체 프로세스가 블로킹됩니다(a).
2. 한 번에 하나의 스레드만 커널에서 실행할 수 있어 멀티코어 활용이 불가능합니다(b, c).
3. 실질적인 병행성(동시성)이 제한됩니다(d).
4. 이로 인해 멀티스레드 애플리케이션의 성능이 저하될 수 있습니다(e).

현재는 대부분의 현대 운영체제가 One-to-One 모델이나 하이브리드 모델을 사용하고 있으며, Many-to-One 모델은 거의 사용되지 않습니다. Many-to-One 모델의 예로는 자료에서 언급된 Solaris Green Threads와 GNU Portable Threads가 있습니다.

## 문제 7: 네이티브 프로세스 핸들 이해하기

Windows 운영체제에서 다음 코드의 문제점은 무엇인가?

```c
// 첫 번째 프로세스에서 실행
HANDLE hObjInProcessS = CreateMutex(NULL, FALSE, NULL);
HANDLE hProcessT = OpenProcess(PROCESS_ALL_ACCESS, FALSE, dwProcessIdT);
HANDLE hObjInProcessT; // 초기화되지 않은 핸들

// 프로세스 T에게 뮤텍스 객체 접근 권한 부여
DuplicateHandle(GetCurrentProcess(), hObjInProcessS, 
               hProcessT, &hObjInProcessT, 
               0, FALSE, DUPLICATE_SAME_ACCESS);

// 프로세스 T와의 통신이 더 이상 필요 없음
CloseHandle(hProcessT);

// 뮤텍스 사용이 끝나면 핸들 닫기
CloseHandle(hObjInProcessS);

// 문제가 있는 코드
CloseHandle(hObjInProcessT);
```

a) `DuplicateHandle` 호출이 잘못됨
b) `OpenProcess` 호출에 잘못된 접근 권한이 지정됨
c) 다른 프로세스의 핸들을 닫으려고 시도함
d) 뮤텍스가 아직 사용 중일 수 있음
e) 핸들이 닫히는 순서가 잘못됨

**해설**: 정답은 **c) 다른 프로세스의 핸들을 닫으려고 시도함**입니다. 코드의 마지막 줄에서 `CloseHandle(hObjInProcessT)`는 프로세스 T의 핸들 테이블에 있는 핸들을 닫으려고 시도합니다. 그러나 `hObjInProcessT`는 현재 프로세스(S)의 컨텍스트에서는 의미가 없는 값입니다.

`DuplicateHandle`을 호출하면 소스 프로세스(S)의 핸들 테이블 항목이 대상 프로세스(T)의 핸들 테이블로 복사됩니다. 결과적으로 `hObjInProcessT`는 T의 핸들 테이블 내의 인덱스일 뿐, S에서 직접 사용할 수 있는 핸들이 아닙니다.

이 코드가 실행되면 S가 우연히 `hObjInProcessT` 값에 해당하는 유효한 핸들을 가지고 있다면, 엉뚱한 커널 객체가 닫힐 수 있습니다. 이는 예측할 수 없는 동작과 잠재적인 애플리케이션 충돌을 유발할 수 있습니다.

## 문제 8: 스레드 종료 방식

Windows에서 스레드를 종료하는 가장 안전한 방법은?

a) `TerminateThread()` 함수 호출
b) `ExitThread()` 함수 호출
c) 스레드 함수에서 리턴
d) 프로세스 종료하기
e) `SuspendThread()` 함수 호출 후 스레드 핸들 닫기

**해설**: 정답은 **c) 스레드 함수에서 리턴**입니다. Windows에서 스레드를 종료하는 네 가지 방법 중에서 스레드 함수가 반환되도록 하는 것이 가장 안전하고 권장되는 방법입니다. 이유는 다음과 같습니다:

1. 스레드 함수가 반환되면 모든 C++ 객체가 소멸자를 통해 올바르게 정리됩니다.
2. 스레드의 스택에 할당된 메모리가 올바르게 해제됩니다.
3. 운영체제가 스레드의 종료 코드를 스레드 함수의 반환값으로 설정합니다.
4. 스레드 커널 객체의 사용 카운트가 감소합니다.

반면, 다른 방법들은 문제가 있습니다:
- `ExitThread()`는 C++ 리소스를 정리하지 않습니다(대신 C++ 코드에서는 `_endthreadex()` 사용).
- `TerminateThread()`는 더 위험하며 스레드 리소스를 정리하지 않습니다.
- 프로세스 종료는 모든 스레드를 강제 종료시키는 극단적인 방법입니다.
- `SuspendThread()`는 스레드를 종료하지 않고 일시 중단만 합니다.

## 문제 9: 원형 버퍼(Circular Buffer) 용량

다음 코드로 구현된 원형 버퍼에서 `BUFFER_SIZE`가 10으로 정의되어 있을 때, 실제로 저장할 수 있는 최대 항목 수는?

```c
// 생산자 코드
while ((in + 1) % BUFFER_SIZE == out)
    ; /* do nothing */
buffer[in] = next_produced;
in = (in + 1) % BUFFER_SIZE;

// 소비자 코드
while (in == out)
    ; /* do nothing */
next_consumed = buffer[out];
out = (out + 1) % BUFFER_SIZE;
```

a) 8개
b) 9개
c) 10개
d) 무제한
e) 컴파일러에 따라 다름

**해설**: 정답은 **b) 9개**입니다. 이 코드는 전형적인 생산자-소비자 패턴의 원형 버퍼 구현입니다. 버퍼가 가득 찼는지 확인하는 조건은 `(in + 1) % BUFFER_SIZE == out`입니다.

이 구현에서는 항상 하나의 슬롯을 비워 두어야 합니다. 그 이유는:

1. 버퍼가 비어 있는 상태는 `in == out`입니다.
2. 버퍼가 가득 찬 상태를 감지하기 위해 `in`이 `out` 바로 앞에 있는지 확인합니다(`(in + 1) % BUFFER_SIZE == out`).
3. 만약 모든 10개의 슬롯을 다 사용한다면, 버퍼가 가득 찼을 때와 비어 있을 때 모두 `in == out` 조건이 성립하게 되어 구분할 수 없게 됩니다.

따라서 `BUFFER_SIZE`가 10일 때, 실제로 사용할 수 있는 최대 항목 수는 9개입니다. 이것은 원형 버퍼 구현에서 흔히 발생하는 제약 사항입니다.

## 문제 10: 공유 메모리의 포인터 문제

다음과 같이 공유 메모리를 사용하는 코드의 문제점은 무엇인가?

```c
// 프로세스 A에서 실행
shared_memory_segment *shm = attach_shared_memory();
item *p = &shm->buffer[3];
// 프로세스 B에 포인터 p를 전달

// 프로세스 B에서 실행
// 프로세스 A로부터 포인터 p를 받음
*p = 42;  // 이 코드의 문제점은?
```

a) Race condition이 발생할 수 있음
b) 메모리 누수가 발생할 수 있음
c) 다른 프로세스의 가상 주소 공간에서 포인터가 유효하지 않을 수 있음
d) 버퍼 오버플로우 가능성
e) 공유 메모리는 포인터를 저장할 수 없음

**해설**: 정답은 **c) 다른 프로세스의 가상 주소 공간에서 포인터가 유효하지 않을 수 있음**입니다. 공유 메모리 세그먼트는 각 프로세스의 가상 주소 공간에서 다른 주소에 매핑될 수 있습니다. 따라서 프로세스 A에서 유효한 포인터 `p`가 프로세스 B에서는 전혀 다른 메모리 위치를 가리킬 수 있습니다.

이 문제를 해결하는 방법은:
1. 절대 주소 대신 상대적 오프셋을 사용하기: `size_t offset = (char*)p - (char*)shm;`
2. 다른 프로세스에서: `p = (item*)((char*)shm + offset);`
3. 또는 인덱스 번호를 사용하여 참조하기: `index = 3; shm->buffer[index] = 42;`

이렇게 하면 공유 메모리가 각 프로세스의 주소 공간에 어디에 매핑되든 상관없이 올바른 데이터에 접근할 수 있습니다. 이것은 공유 메모리를 사용할 때 자주 발생하는 실수 중 하나입니다.

## 문제 11: 멀티코어 시스템에서의 동기화

다음 코드는 멀티코어 시스템에서 어떤 문제를 일으킬 수 있는가?

```c
// 임계 영역(critical section) 진입을 위한 락 획득
while (TestAndSet(&lock) == 1)
    ; // 계속 시도
// 임계 영역 코드
// ...
// 락 해제
lock = 0;
```

a) 기아 상태(Starvation)
b) 우선순위 역전(Priority inversion)
c) 캐시 스래싱(Cache thrashing)
d) 교착 상태(Deadlock)
e) 메모리 누수(Memory leak)

**해설**: 정답은 **c) 캐시 스래싱(Cache thrashing)**입니다. 멀티코어 시스템에서 위 코드와 같이 락을 획득하기 위해 계속 TSL(Test-and-Set Lock) 명령을 수행하면, 각 CPU가 `lock` 변수의 캐시 라인을 지속적으로 무효화하고 갱신하는 과정이 발생합니다.

이런 현상이 발생하는 이유는:
1. TSL 명령은 쓰기 연산(write operation)을 포함하기 때문에 락의 값을 확인하면서 동시에 수정을 시도합니다.
2. 한 CPU가 TSL 명령을 실행하면 해당 캐시 라인의 배타적 소유권(exclusive ownership)을 획득합니다.
3. 다른 CPU가 같은 락에 대해 TSL을 실행하려면, 이전 CPU의 캐시 라인을 무효화하고 메모리에서 새로 가져와야 합니다.
4. 이러한 과정이 반복되면서 캐시 라인이 CPU 간에 계속 이동하는 "캐시 스래싱" 현상이 발생합니다.

이 문제를 완화하는 방법으로는:
1. 먼저 read-only 확인 후 락이 해제되었을 때만 TSL 시도하기
2. 지수적 백오프(exponential backoff) 알고리즘 적용하기
3. 각 CPU마다 개별 락 변수를 사용하는 방식(MCS lock과 같은) 채택하기

이는 멀티코어/멀티프로세서 시스템에서 동기화를 구현할 때 발생하는 성능 관련 문제 중 하나입니다.

## 문제 12: NUMA 아키텍처

NUMA(Non-Uniform Memory Access) 아키텍처에 대한 설명으로 틀린 것은?

a) 모든 메모리 모듈에 접근하는 시간이 동일하다
b) 단일 주소 공간을 모든 CPU에 제공한다
c) 원격 메모리 접근은 LOAD 및 STORE 명령으로 이루어진다
d) 로컬 메모리 접근보다 원격 메모리 접근이 더 느리다
e) 확장성이 좋아 수백 개의 CPU로 구성된 시스템을 구축할 수 있다

**해설**: 정답은 **a) 모든 메모리 모듈에 접근하는 시간이 동일하다**입니다. 이는 NUMA 아키텍처의 특성과 정반대입니다. NUMA의 핵심 특징은:

1. 메모리 접근 시간이 균일하지 않음(Non-Uniform): 로컬 메모리 접근이 원격 메모리 접근보다 빠릅니다.
2. 모든 CPU에게 단일 주소 공간 제공: 모든 CPU는 모든 메모리에 접근 가능합니다.
3. 원격 메모리에 대한 접근은 일반 LOAD/STORE 명령을 통해 이루어집니다.

NUMA는 메모리 모듈을 CPU와 가까운 위치에 배치하여 로컬 메모리 접근을 빠르게 하면서도, 필요시 다른 CPU의 메모리(원격 메모리)에도 접근할 수 있게 합니다. 이 아키텍처는 많은 CPU를 가진 대규모 시스템에서 메모리 접근 병목 현상을 완화하는 데 유용합니다. UMA(Uniform Memory Access) 아키텍처는 모든 메모리 접근 시간이 동일한 반면, NUMA는 그렇지 않습니다.

## 문제 13: 스레드 전환 vs 컨텍스트 전환

"Thread switching has lower overhead than context switching"이라는 문장의 의미로 가장 정확한 것은?

a) 스레드 간 전환이 프로세스 간 전환보다 비용이 적게 든다
b) 커널 모드에서 사용자 모드로 전환하는 것보다 스레드 전환이 더 빠르다
c) 한 CPU에서 다른 CPU로 전환하는 것보다 스레드 전환이 더 효율적이다
d) 인터럽트 처리가 스레드 전환보다 더 많은 오버헤드를 발생시킨다
e) 하이퍼스레딩은 컨텍스트 전환보다 효율적이다

**해설**: 정답은 **a) 스레드 간 전환이 프로세스 간 전환보다 비용이 적게 든다**입니다. 이 문장에서 "thread switching"은 같은 프로세스 내의 스레드 간 전환을, "context switching"은 서로 다른 프로세스 간의 전환을 의미합니다.

스레드 전환이 프로세스 전환보다 오버헤드가 적은 이유는:
1. 같은 프로세스 내 스레드들은 주소 공간을 공유하므로 메모리 관련 컨텍스트(페이지 테이블 등)를 변경할 필요가 없습니다.
2. 프로세스 전환은 TLB(Translation Lookaside Buffer)를 무효화해야 하지만, 스레드 전환은 그럴 필요가 없습니다.
3. 캐시 무효화가 프로세스 전환에서 더 광범위하게 발생할 수 있습니다.

이러한 효율성은 멀티스레딩의 주요 장점 중 하나로, 자료의 "경제성(Economy)" 섹션에서 "thread switching lower overhead than context switching"으로 언급되어 있습니다.

## 문제 14: 멀티스레딩의 장점

다음 중 멀티스레딩의 장점으로 볼 수 없는 것은?

a) 응답성(Responsiveness) - 프로세스의 일부가 블로킹되어도 계속 실행할 수 있음
b) 자원 공유(Resource Sharing) - 스레드는 프로세스의 자원을 공유함
c) 경제성(Economy) - 스레드 생성이 프로세스 생성보다 저렴함
d) 확장성(Scalability) - 멀티프로세서 아키텍처를 활용할 수 있음
e) 보안성(Security) - 다른 프로세스의 메모리에 접근할 수 없음

**해설**: 정답은 **e) 보안성(Security) - 다른 프로세스의 메모리에 접근할 수 없음**입니다. 이것은 멀티스레딩의 장점이 아니라 별도 프로세스를 사용할 때의 장점입니다.

멀티스레딩의 주요 장점은:
1. **응답성(Responsiveness)**: 한 스레드가 블로킹되어도 다른 스레드는 계속 실행될 수 있어 사용자 인터페이스의 응답성을 유지할 수 있습니다.
2. **자원 공유(Resource Sharing)**: 같은 프로세스 내 스레드들은 코드, 데이터, 파일 등을 자연스럽게 공유합니다.
3. **경제성(Economy)**: 스레드 생성과 전환은 프로세스보다 적은 오버헤드가 듭니다.
4. **확장성(Scalability)**: 멀티코어 환경에서 병렬 처리를 통해 성능을 향상시킬 수 있습니다.

반면, 보안성은 오히려 멀티스레딩의 단점일 수 있습니다. 같은 프로세스 내 스레드는 모두 같은 메모리 공간을 공유하므로, 한 스레드의 오류가 다른 스레드에 영향을 미칠 수 있습니다. 별도의 프로세스를 사용하면 프로세스 간 메모리 보호를 통해 더 나은 격리와 보안을 제공할 수 있습니다.

## 문제 15: 스핀 락 vs 스레드 전환

멀티코어 시스템에서 뮤텍스가 잠겨 있을 때, 다음 중 스핀 락(spin lock)이 스레드 전환(thread switching)보다 효율적인 경우는?

a) 뮤텍스가 장시간(10ms 이상) 잠겨 있을 것으로 예상될 때
b) 뮤텍스가 매우 짧은 시간(50μs 미만) 동안만 잠겨 있을 것으로 예상될 때
c) 시스템의 로드가 매우 높을 때
d) 스레드의 우선순위가 낮을 때
e) 메모리 사용량이 중요한 고려사항일 때

**해설**: 정답은 **b) 뮤텍스가 매우 짧은 시간(50μs 미만) 동안만 잠겨 있을 것으로 예상될 때**입니다. 스핀 락과 스레드 전환 중 어느 것이 더 효율적인지는 뮤텍스가 잠겨 있는 예상 시간과 컨텍스트 전환의 오버헤드를 비교해서 결정해야 합니다.

스레드 전환의 비용은 상당히 높습니다:
1. 현재 스레드의 상태를 저장해야 합니다.
2. 새 스레드를 선택하고 그 상태를 로드해야 합니다.
3. 캐시 미스와 TLB 미스가 발생할 수 있습니다.
4. 나중에 원래 스레드로 다시 전환해야 합니다.

이러한 오버헤드는 보통 수 마이크로초에서 수십 마이크로초가 소요됩니다. 따라서:
- 만약 뮤텍스가 이보다 짧은 시간(예: 50μs 미만) 동안만 잠겨 있을 것으로 예상된다면, CPU 사이클을 조금 낭비하더라도 스핀 락을 사용하는 것이 더 효율적입니다.
- 반면, 뮤텍스가 오랫동안(예: 수 밀리초 이상) 잠겨 있을 것으로 예상된다면, 스레드를 블록하고 다른 스레드로 전환하는 것이 CPU 자원을 더 효율적으로 사용하는 방법입니다.

이 트레이드오프는 검색 결과에서 언급된 내용으로, 스레드 동기화 메커니즘 설계에서 중요한 고려사항입니다.
